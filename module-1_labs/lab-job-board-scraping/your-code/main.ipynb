{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Job Board Scraping Lab\n",
    "\n",
    "In this lab you will first see a minimal but fully functional code snippet to scrape the LinkedIn Job Search webpage. You will then work on top of the example code and complete several chanllenges.\n",
    "\n",
    "### Some Resources \n",
    "\n",
    "- [Requests library](http://docs.python-requests.org/en/master/#the-user-guide) documentation \n",
    "- [Beautiful Soup Doc](https://www.crummy.com/software/BeautifulSoup/bs4/doc/)\n",
    "- [Urllib](https://docs.python.org/3/library/urllib.html#module-urllib)\n",
    "- [re lib](https://docs.python.org/3/library/re.html)\n",
    "- [Scrapy](https://scrapy.org/)\n",
    "- [List of HTTP status codes](https://en.wikipedia.org/wiki/List_of_HTTP_status_codes)\n",
    "- [HTML basics](http://www.simplehtmlguide.com/cheatsheet.php)\n",
    "- [CSS basics](https://www.cssbasics.com/#page_start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the required libraries\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "\"\"\"\n",
    "This function searches job posts from LinkedIn and converts the results into a dataframe.\n",
    "\"\"\"\n",
    "def scrape_linkedin_job_search(keywords):\n",
    "    \n",
    "    # Define the base url to be scraped.\n",
    "    # All uppercase variable name signifies this is a constant and its value should never unchange\n",
    "    BASE_URL = 'https://www.linkedin.com/jobs/search/?'\n",
    "        \n",
    "    # Assemble the full url with parameters\n",
    "    scrape_url = ''.join([BASE_URL, 'keywords=', keywords])\n",
    "\n",
    "    # Create a request to get the data from the server \n",
    "    page = requests.get(scrape_url)\n",
    "    soup = BeautifulSoup(page.text, 'html.parser')\n",
    "    # Create an empty dataframe with the columns consisting of the information you want to capture\n",
    "    columns = ['Title', 'Company', 'Location']\n",
    "    data = pd.DataFrame(columns=columns)\n",
    "\n",
    "    # Retrieve HTML code from the webpage. Parse the HTML into a list of \"cards\".\n",
    "    # Then in each job card, extract the job title, company, and location data.\n",
    "    titles = []\n",
    "    companies = []\n",
    "    locations = []\n",
    "    for card in soup.select(\"div.result-card__contents\"):\n",
    "        title = card.findChild(\"h3\", recursive=False)\n",
    "        company = card.findChild(\"h4\", recursive=False)\n",
    "        location = card.findChild(\"span\", attrs={\"class\": \"job-result-card__location\"}, recursive=True)\n",
    "        titles.append(title.string)\n",
    "        companies.append(company.string)\n",
    "        locations.append(location.string)\n",
    "\n",
    "    # Inject job titles, companies, and locations into the empty dataframe\n",
    "    zipped = zip(titles, companies, locations)\n",
    "    for z in list(zipped):\n",
    "        data=data.append({'Title' : z[0] , 'Company' : z[1], 'Location': z[2]} , ignore_index=True)\n",
    "\n",
    "    # Return dataframe\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Company</th>\n",
       "      <th>Location</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>IT Senior Associate</td>\n",
       "      <td>Kearney &amp; Company</td>\n",
       "      <td>McLean, VA, US</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>Intern: Data Analysis/Process Improvement Prog...</td>\n",
       "      <td>Juniper Networks</td>\n",
       "      <td>Sunnyvale, CA, US</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>Associate / Business Analyst (Consulting Data ...</td>\n",
       "      <td>Foresight Associates, LLC</td>\n",
       "      <td>Greater Minneapolis-St. Paul Area</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>Software Engineer, Data Models &amp; Analysis</td>\n",
       "      <td>Improbable</td>\n",
       "      <td>Washington, D.C., DC, US</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>Manager, Reporting and Data Analysis</td>\n",
       "      <td>The Madison Square Garden Company</td>\n",
       "      <td>Greater New York City Area</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>Data Analysis Manager</td>\n",
       "      <td>Capital One</td>\n",
       "      <td>McLean, VA, US</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>Data Management and Analysis Services Analyst I</td>\n",
       "      <td>Conduent</td>\n",
       "      <td>Ocoee, Florida</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>Manager, Data Analysis &amp; Insights</td>\n",
       "      <td>The Coca-Cola Company</td>\n",
       "      <td>Atlanta, GA, US</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>Viper Staffing Services L.L.C.</td>\n",
       "      <td>Glendale, California, United States</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>Sr. Bioinformatics Scientist</td>\n",
       "      <td>The EAC Agency</td>\n",
       "      <td>Fremont, California, United States</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>Programmer Anaylyst</td>\n",
       "      <td>Valley First Credit Union</td>\n",
       "      <td>Modesto, California Area</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>Data Analysis Coordinator</td>\n",
       "      <td>The Job Network</td>\n",
       "      <td>Troy, NY, US</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>Sr. Manager, Strategic Business Intelligence &amp;...</td>\n",
       "      <td>Project Assistants</td>\n",
       "      <td>Charlotte, NC, US</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>Operations Manager</td>\n",
       "      <td>California Umbrella</td>\n",
       "      <td>92509, Riverside, California, United States</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>DataStage Developer</td>\n",
       "      <td>Confidential</td>\n",
       "      <td>Bellevue, Washington, United States</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>Sr. Systems Engineer-Data Analysis</td>\n",
       "      <td>Vprecruiter</td>\n",
       "      <td>Bethesda, MD, US</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>Scientist</td>\n",
       "      <td>Nantero</td>\n",
       "      <td>Woburn, Massachusetts</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>Procurement Executive – Data Analysis (Contract)</td>\n",
       "      <td>Blue Signal Search</td>\n",
       "      <td>Decatur, IL, US</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>Manager, Data and Analysis</td>\n",
       "      <td>KUOW Public Radio</td>\n",
       "      <td>Seattle, WA, US</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>Manager of Data Analysis</td>\n",
       "      <td>Summit Public Schools</td>\n",
       "      <td>Redwood City, CA, US</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>Data Management, Analysis &amp; Reporting</td>\n",
       "      <td>Bank of America</td>\n",
       "      <td>Newark, DE, US</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>Data Analysis Manager</td>\n",
       "      <td>Genworth</td>\n",
       "      <td>Richmond, VA, US</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>Data Analysis</td>\n",
       "      <td>ClearedJobs.Net</td>\n",
       "      <td>Redstone Arsenal, AL, US</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>Automation Engineer - Data Analysis/Simulation</td>\n",
       "      <td>Wynright Corporation</td>\n",
       "      <td>Bolingbrook, IL, US</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>Data Analysis Manager</td>\n",
       "      <td>Summit Public Schools (Public Charter Network)</td>\n",
       "      <td>Redwood City, CA, US</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Title  \\\n",
       "0                                 IT Senior Associate   \n",
       "1   Intern: Data Analysis/Process Improvement Prog...   \n",
       "2   Associate / Business Analyst (Consulting Data ...   \n",
       "3           Software Engineer, Data Models & Analysis   \n",
       "4                Manager, Reporting and Data Analysis   \n",
       "5                               Data Analysis Manager   \n",
       "6     Data Management and Analysis Services Analyst I   \n",
       "7                   Manager, Data Analysis & Insights   \n",
       "8                                        Data Analyst   \n",
       "9                        Sr. Bioinformatics Scientist   \n",
       "10                                Programmer Anaylyst   \n",
       "11                          Data Analysis Coordinator   \n",
       "12  Sr. Manager, Strategic Business Intelligence &...   \n",
       "13                                 Operations Manager   \n",
       "14                                DataStage Developer   \n",
       "15                 Sr. Systems Engineer-Data Analysis   \n",
       "16                                          Scientist   \n",
       "17   Procurement Executive – Data Analysis (Contract)   \n",
       "18                         Manager, Data and Analysis   \n",
       "19                           Manager of Data Analysis   \n",
       "20              Data Management, Analysis & Reporting   \n",
       "21                              Data Analysis Manager   \n",
       "22                                      Data Analysis   \n",
       "23     Automation Engineer - Data Analysis/Simulation   \n",
       "24                              Data Analysis Manager   \n",
       "\n",
       "                                           Company  \\\n",
       "0                                Kearney & Company   \n",
       "1                                 Juniper Networks   \n",
       "2                        Foresight Associates, LLC   \n",
       "3                                       Improbable   \n",
       "4                The Madison Square Garden Company   \n",
       "5                                      Capital One   \n",
       "6                                         Conduent   \n",
       "7                            The Coca-Cola Company   \n",
       "8                   Viper Staffing Services L.L.C.   \n",
       "9                                   The EAC Agency   \n",
       "10                       Valley First Credit Union   \n",
       "11                                 The Job Network   \n",
       "12                              Project Assistants   \n",
       "13                             California Umbrella   \n",
       "14                                    Confidential   \n",
       "15                                     Vprecruiter   \n",
       "16                                         Nantero   \n",
       "17                              Blue Signal Search   \n",
       "18                               KUOW Public Radio   \n",
       "19                           Summit Public Schools   \n",
       "20                                 Bank of America   \n",
       "21                                        Genworth   \n",
       "22                                 ClearedJobs.Net   \n",
       "23                            Wynright Corporation   \n",
       "24  Summit Public Schools (Public Charter Network)   \n",
       "\n",
       "                                       Location  \n",
       "0                                McLean, VA, US  \n",
       "1                             Sunnyvale, CA, US  \n",
       "2             Greater Minneapolis-St. Paul Area  \n",
       "3                      Washington, D.C., DC, US  \n",
       "4                    Greater New York City Area  \n",
       "5                                McLean, VA, US  \n",
       "6                                Ocoee, Florida  \n",
       "7                               Atlanta, GA, US  \n",
       "8           Glendale, California, United States  \n",
       "9            Fremont, California, United States  \n",
       "10                     Modesto, California Area  \n",
       "11                                 Troy, NY, US  \n",
       "12                            Charlotte, NC, US  \n",
       "13  92509, Riverside, California, United States  \n",
       "14          Bellevue, Washington, United States  \n",
       "15                             Bethesda, MD, US  \n",
       "16                        Woburn, Massachusetts  \n",
       "17                              Decatur, IL, US  \n",
       "18                              Seattle, WA, US  \n",
       "19                         Redwood City, CA, US  \n",
       "20                               Newark, DE, US  \n",
       "21                             Richmond, VA, US  \n",
       "22                     Redstone Arsenal, AL, US  \n",
       "23                          Bolingbrook, IL, US  \n",
       "24                         Redwood City, CA, US  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example to call the function\n",
    "\n",
    "results = scrape_linkedin_job_search('data%20analysis')\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge 1\n",
    "\n",
    "The first challenge for you is to update the `scrape_linkedin_job_search` function by adding a new parameter called `num_pages`. This will allow you to search more than 25 jobs with this function. Suggested steps:\n",
    "\n",
    "1. Go to https://www.linkedin.com/jobs/search/?keywords=data%20analysis in your browser.\n",
    "1. Scroll down the left panel and click the page 2 link. Look at how the URL changes and identify the page offset parameter.\n",
    "1. Add `num_pages` as a new param to the `scrape_linkedin_job_search` function. Update the function code so that it uses a \"for\" loop to retrieve several pages of search results.\n",
    "1. Test your new function by scraping 5 pages of the search results.\n",
    "\n",
    "Hint: Prepare for the case where there are less than 5 pages of search results. Your function should be robust enough to **not** trigger errors. Simply skip making additional searches and return all results if the search already reaches the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the required libraries\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "\"\"\"\n",
    "This function searches job posts from LinkedIn and converts the results into a dataframe.\n",
    "\"\"\"\n",
    "def scrape_linkedin_job_search2(keywords,numpages):\n",
    "    \n",
    "    # Define the base url to be scraped.\n",
    "    # All uppercase variable name signifies this is a constant and its value should never unchange\n",
    "    BASE_URL = 'https://www.linkedin.com/jobs/search/?'\n",
    "    \n",
    "    # Create an empty dataframe with the columns consisting of the information you want to capture\n",
    "    columns = ['Title', 'Company', 'Location']\n",
    "    data = pd.DataFrame(columns=columns)\n",
    "    #https://www.linkedin.com/jobs/search/?geoId=101282230&keywords=data%20analysis&location=Germany&start=25\n",
    "    for page in range(numpages):\n",
    "        numpages_num=page*25\n",
    "        # Assemble the full url with parameters\n",
    "        scrape_url = ''.join([BASE_URL, 'keywords=', keywords,'&start=',str(numpages_num)])\n",
    "\n",
    "        # Create a request to get the data from the server \n",
    "        page = requests.get(scrape_url)\n",
    "        soup = BeautifulSoup(page.text, 'html.parser')\n",
    "\n",
    "        # Retrieve HTML code from the webpage. Parse the HTML into a list of \"cards\".\n",
    "        # Then in each job card, extract the job title, company, and location data.\n",
    "        titles = []\n",
    "        companies = []\n",
    "        locations = []\n",
    "        for card in soup.select(\"div.result-card__contents\"):\n",
    "            title = card.findChild(\"h3\", recursive=False)\n",
    "            company = card.findChild(\"h4\", recursive=False)\n",
    "            location = card.findChild(\"span\", attrs={\"class\": \"job-result-card__location\"}, recursive=True)\n",
    "            titles.append(title.string)\n",
    "            companies.append(company.string)\n",
    "            locations.append(location.string)\n",
    "\n",
    "        # Inject job titles, companies, and locations into the empty dataframe\n",
    "        zipped = zip(titles, companies, locations)\n",
    "        for z in list(zipped):\n",
    "            data=data.append({'Title' : z[0] , 'Company' : z[1], 'Location': z[2]} , ignore_index=True)\n",
    "\n",
    "    # Return dataframe\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Company</th>\n",
       "      <th>Location</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>Sr. Data Analyst – Predictive &amp; Inferential An...</td>\n",
       "      <td>Peyton Resource Group</td>\n",
       "      <td>Arlington, Texas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>Manager, Reporting and Data Analysis</td>\n",
       "      <td>The Madison Square Garden Company</td>\n",
       "      <td>Greater New York City Area</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>Data Analysis Manager</td>\n",
       "      <td>Addison Group</td>\n",
       "      <td>Houston, TX, US</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>Manager, Data and Analysis</td>\n",
       "      <td>Digitas North America</td>\n",
       "      <td>Greater New York City Area</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>Intern: Data Analysis/Process Improvement Prog...</td>\n",
       "      <td>Juniper Networks</td>\n",
       "      <td>Sunnyvale, CA, US</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>24/7 Wall St.</td>\n",
       "      <td>Greater New York City Area</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>121</td>\n",
       "      <td>Senior Data Analyst</td>\n",
       "      <td>Trexin Consulting</td>\n",
       "      <td>Chicago, Illinois</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>122</td>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>24 Seven LLC</td>\n",
       "      <td>Portland, Oregon</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>123</td>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>Partner's Consulting, Inc.</td>\n",
       "      <td>Philadelphia, Pennsylvania</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>124</td>\n",
       "      <td>Principal Data Analyst, Flight Operations Anal...</td>\n",
       "      <td>Delta Air Lines</td>\n",
       "      <td>Atlanta, GA, US</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>125 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 Title  \\\n",
       "0    Sr. Data Analyst – Predictive & Inferential An...   \n",
       "1                 Manager, Reporting and Data Analysis   \n",
       "2                                Data Analysis Manager   \n",
       "3                           Manager, Data and Analysis   \n",
       "4    Intern: Data Analysis/Process Improvement Prog...   \n",
       "..                                                 ...   \n",
       "120                                       Data Analyst   \n",
       "121                                Senior Data Analyst   \n",
       "122                                       Data Analyst   \n",
       "123                                       Data Analyst   \n",
       "124  Principal Data Analyst, Flight Operations Anal...   \n",
       "\n",
       "                               Company                    Location  \n",
       "0                Peyton Resource Group            Arlington, Texas  \n",
       "1    The Madison Square Garden Company  Greater New York City Area  \n",
       "2                        Addison Group             Houston, TX, US  \n",
       "3                Digitas North America  Greater New York City Area  \n",
       "4                     Juniper Networks           Sunnyvale, CA, US  \n",
       "..                                 ...                         ...  \n",
       "120                      24/7 Wall St.  Greater New York City Area  \n",
       "121                  Trexin Consulting           Chicago, Illinois  \n",
       "122                       24 Seven LLC            Portland, Oregon  \n",
       "123         Partner's Consulting, Inc.  Philadelphia, Pennsylvania  \n",
       "124                    Delta Air Lines             Atlanta, GA, US  \n",
       "\n",
       "[125 rows x 3 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# your code here\n",
    "results2 = scrape_linkedin_job_search2('data%20analysis',5)\n",
    "results2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge 2\n",
    "\n",
    "Further improve your function so that it can search jobs in a specific country. Add the 3rd param to your function called `country`. The steps are identical to those in Challange 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "# Import the required libraries\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "\"\"\"\n",
    "This function searches job posts from LinkedIn and converts the results into a dataframe.\n",
    "\"\"\"\n",
    "def scrape_linkedin_job_search3(keywords,numpages,country):\n",
    "    \n",
    "    # Define the base url to be scraped.\n",
    "    # All uppercase variable name signifies this is a constant and its value should never unchange\n",
    "    BASE_URL = 'https://www.linkedin.com/jobs/search/?'\n",
    "    \n",
    "    # Create an empty dataframe with the columns consisting of the information you want to capture\n",
    "    columns = ['Title', 'Company', 'Location']\n",
    "    data = pd.DataFrame(columns=columns)\n",
    "    \n",
    "    for x in range(numpages):\n",
    "        numpages_num=x*25\n",
    "        # Assemble the full url with parameters\n",
    "        scrape_url = ''.join([BASE_URL, 'keywords=', keywords,'&start=',str(numpages_num),'&location=',country])\n",
    "\n",
    "        # Create a request to get the data from the server \n",
    "        page = requests.get(scrape_url)\n",
    "        soup = BeautifulSoup(page.text, 'html.parser')\n",
    "\n",
    "        # Retrieve HTML code from the webpage. Parse the HTML into a list of \"cards\".\n",
    "        # Then in each job card, extract the job title, company, and location data.\n",
    "        titles = []\n",
    "        companies = []\n",
    "        locations = []\n",
    "        for card in soup.select(\"div.result-card__contents\"):\n",
    "            title = card.findChild(\"h3\", recursive=False)\n",
    "            company = card.findChild(\"h4\", recursive=False)\n",
    "            location = card.findChild(\"span\", attrs={\"class\": \"job-result-card__location\"}, recursive=True)\n",
    "            titles.append(title.string)\n",
    "            companies.append(company.string)\n",
    "            locations.append(location.string)\n",
    "\n",
    "        # Inject job titles, companies, and locations into the empty dataframe\n",
    "        zipped = zip(titles, companies, locations)\n",
    "        for z in list(zipped):\n",
    "            data=data.append({'Title' : z[0] , 'Company' : z[1], 'Location': z[2]} , ignore_index=True)\n",
    "\n",
    "    # Return dataframe\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Company</th>\n",
       "      <th>Location</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>Senior Data Analyst</td>\n",
       "      <td>Takeaway.com</td>\n",
       "      <td>Berlin, DE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>Head of Risk Analytics &amp; Data Science (all gen...</td>\n",
       "      <td>Spotcap Global</td>\n",
       "      <td>Berlin und Umgebung, Deutschland</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>Senior Data Analyst</td>\n",
       "      <td>komoot</td>\n",
       "      <td>Berlin, Berlin, Germany</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>Senior Data Analyst</td>\n",
       "      <td>N26</td>\n",
       "      <td>Berlin, DE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>Amazon</td>\n",
       "      <td>Munich, DE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>Data Analyst - Finance</td>\n",
       "      <td>Jimdo</td>\n",
       "      <td>Hamburg, Hamburg, Germany</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>Data Science Intern</td>\n",
       "      <td>AiSight</td>\n",
       "      <td>Berlin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>Euro London Appointments</td>\n",
       "      <td>Berlin, Berlin, Germany</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>Project Management Analyst (Intern)</td>\n",
       "      <td>Adobe</td>\n",
       "      <td>Hamburg, DE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>Senior Data Scientist</td>\n",
       "      <td>Glocomms</td>\n",
       "      <td>Stuttgart Area, Germany</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>Senior Data Scientist in Field Trial Analytics...</td>\n",
       "      <td>Bayer</td>\n",
       "      <td>Monheim am Rhein, DE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>Senior Data Analyst (M/ F/ X)</td>\n",
       "      <td>The REC HUB</td>\n",
       "      <td>Berlin Area, Germany</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>Data Engineer</td>\n",
       "      <td>ICIS</td>\n",
       "      <td>Karlsruhe, Baden-Württemberg, Germany</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>GetYourGuide</td>\n",
       "      <td>Berlin, DE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>Data Analyst - Marketing Intelligence</td>\n",
       "      <td>trivago</td>\n",
       "      <td>Düsseldorf, DE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>Engineer R&amp;D Data Science</td>\n",
       "      <td>Alcon</td>\n",
       "      <td>Berlin, DE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>Consumer Research &amp; Data Analyst</td>\n",
       "      <td>Mintel</td>\n",
       "      <td>Düsseldorf, DE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>Data Scientist /  Big Data Engineer (m/f)</td>\n",
       "      <td>CID GmbH</td>\n",
       "      <td>Freigericht, Hessen, Deutschland</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>Senior Data Scientist</td>\n",
       "      <td>Lab1886</td>\n",
       "      <td>Berlin Area, Germany</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>Data Developer</td>\n",
       "      <td>Movement8</td>\n",
       "      <td>Berlin Area, Germany</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>Sr. Product Manager, Technical - Data Infra</td>\n",
       "      <td>Nubank</td>\n",
       "      <td>Berlin, DE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>Learning &amp; Development Specialist: Data Analys...</td>\n",
       "      <td>HERE Technologies</td>\n",
       "      <td>Berlin, DE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>Advanced Analytics Consultant - Data Science</td>\n",
       "      <td>eClerx</td>\n",
       "      <td>Herzogenaurach, DE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>Senior Data Scientist - Data &amp; Machine Learnin...</td>\n",
       "      <td>Delivery Hero</td>\n",
       "      <td>Berlin, DE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>BI &amp; Data Analytics Projekt-Manager</td>\n",
       "      <td>Airbus</td>\n",
       "      <td>Donauwörth, Bayern, Deutschland</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>Professional Consultant (f/m/d) Procurement Da...</td>\n",
       "      <td>CAMELOT Management Consultants</td>\n",
       "      <td>Mannheim, Baden-Württemberg, Germany</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>Microsoft Analytics Consultant (w/m/x)</td>\n",
       "      <td>Avanade</td>\n",
       "      <td>Düsseldorf, DE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>Senior Data Analyst</td>\n",
       "      <td>Takeaway.com</td>\n",
       "      <td>Berlin, DE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>Data Architect - McKinsey Digital</td>\n",
       "      <td>McKinsey &amp; Company</td>\n",
       "      <td>Berlin, DE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>Professional Consultant (f/m/d) SAP Master Dat...</td>\n",
       "      <td>Camelot ITLab</td>\n",
       "      <td>Mannheim, Baden-Württemberg, Germany</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>Senior Data Analyst</td>\n",
       "      <td>Amazon</td>\n",
       "      <td>Munich, DE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>Deep Learning Engineer- Deformable Object Trac...</td>\n",
       "      <td>Apple</td>\n",
       "      <td>Munich, DE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>Senior Data Scientist in Field Trial Analytics...</td>\n",
       "      <td>Bayer</td>\n",
       "      <td>Monheim am Rhein, DE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>Data Analyst - Finance</td>\n",
       "      <td>Jimdo</td>\n",
       "      <td>Hamburg, Hamburg, Germany</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>Senior Data Analyst</td>\n",
       "      <td>N26</td>\n",
       "      <td>Berlin, DE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>Euro London Appointments</td>\n",
       "      <td>Berlin, Berlin, Germany</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>Data Engineer</td>\n",
       "      <td>Joonko</td>\n",
       "      <td>Berlin, Berlin, Germany</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>Learning &amp; Development Specialist: Data Analys...</td>\n",
       "      <td>HERE Technologies</td>\n",
       "      <td>Berlin, DE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>Big Data Architect (m/f/d)</td>\n",
       "      <td>EPAM Systems</td>\n",
       "      <td>Frankfurt am Main, DE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>Jam City</td>\n",
       "      <td>Berlin, DE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>Cape Analytics</td>\n",
       "      <td>Munich, DE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41</td>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>Global Legal Entity Identifier Foundation (GLEIF)</td>\n",
       "      <td>Frankfurt am Main, DE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42</td>\n",
       "      <td>Scientist Bioinformatics - Ngs Data analysis (...</td>\n",
       "      <td>The Tron Group</td>\n",
       "      <td>Mainz, DE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43</td>\n",
       "      <td>(Senior) Data Analyst (m/f/x)</td>\n",
       "      <td>Lendico Deutschland GmbH</td>\n",
       "      <td>Berlin, DE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44</td>\n",
       "      <td>UX/UI  Product Manager needed for a Health Pro...</td>\n",
       "      <td>NeuroNation - Brain Health Technology</td>\n",
       "      <td>Berlin Area, Germany</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>Senior Data Analyst / Consultant</td>\n",
       "      <td>Agoda</td>\n",
       "      <td>Munich, DE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46</td>\n",
       "      <td>Data Scientist - Lead Scoring and Pricing - Be...</td>\n",
       "      <td>Frontier Car Group</td>\n",
       "      <td>Berlin, DE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47</td>\n",
       "      <td>Senior Manager Data Analysis (m/w/d)</td>\n",
       "      <td>human council gmbh</td>\n",
       "      <td>Munich, DE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>Data-Analyst /Data-Scientist (m/f/d)</td>\n",
       "      <td>Schwäbisch Media</td>\n",
       "      <td>Stuttgart, DE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49</td>\n",
       "      <td>Data Scientist (f/m/d) Computer Vision &amp; AI-su...</td>\n",
       "      <td>Beiersdorf</td>\n",
       "      <td>Hamburg, DE</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Title  \\\n",
       "0                                 Senior Data Analyst   \n",
       "1   Head of Risk Analytics & Data Science (all gen...   \n",
       "2                                 Senior Data Analyst   \n",
       "3                                 Senior Data Analyst   \n",
       "4                                        Data Analyst   \n",
       "5                              Data Analyst - Finance   \n",
       "6                                 Data Science Intern   \n",
       "7                                        Data Analyst   \n",
       "8                 Project Management Analyst (Intern)   \n",
       "9                               Senior Data Scientist   \n",
       "10  Senior Data Scientist in Field Trial Analytics...   \n",
       "11                      Senior Data Analyst (M/ F/ X)   \n",
       "12                                      Data Engineer   \n",
       "13                                       Data Analyst   \n",
       "14              Data Analyst - Marketing Intelligence   \n",
       "15                          Engineer R&D Data Science   \n",
       "16                   Consumer Research & Data Analyst   \n",
       "17          Data Scientist /  Big Data Engineer (m/f)   \n",
       "18                              Senior Data Scientist   \n",
       "19                                     Data Developer   \n",
       "20        Sr. Product Manager, Technical - Data Infra   \n",
       "21  Learning & Development Specialist: Data Analys...   \n",
       "22       Advanced Analytics Consultant - Data Science   \n",
       "23  Senior Data Scientist - Data & Machine Learnin...   \n",
       "24                BI & Data Analytics Projekt-Manager   \n",
       "25  Professional Consultant (f/m/d) Procurement Da...   \n",
       "26             Microsoft Analytics Consultant (w/m/x)   \n",
       "27                                Senior Data Analyst   \n",
       "28                  Data Architect - McKinsey Digital   \n",
       "29  Professional Consultant (f/m/d) SAP Master Dat...   \n",
       "30                                Senior Data Analyst   \n",
       "31  Deep Learning Engineer- Deformable Object Trac...   \n",
       "32  Senior Data Scientist in Field Trial Analytics...   \n",
       "33                             Data Analyst - Finance   \n",
       "34                                Senior Data Analyst   \n",
       "35                                       Data Analyst   \n",
       "36                                      Data Engineer   \n",
       "37  Learning & Development Specialist: Data Analys...   \n",
       "38                         Big Data Architect (m/f/d)   \n",
       "39                                       Data Analyst   \n",
       "40                                       Data Analyst   \n",
       "41                                       Data Analyst   \n",
       "42  Scientist Bioinformatics - Ngs Data analysis (...   \n",
       "43                      (Senior) Data Analyst (m/f/x)   \n",
       "44  UX/UI  Product Manager needed for a Health Pro...   \n",
       "45                   Senior Data Analyst / Consultant   \n",
       "46  Data Scientist - Lead Scoring and Pricing - Be...   \n",
       "47               Senior Manager Data Analysis (m/w/d)   \n",
       "48               Data-Analyst /Data-Scientist (m/f/d)   \n",
       "49  Data Scientist (f/m/d) Computer Vision & AI-su...   \n",
       "\n",
       "                                              Company  \\\n",
       "0                                        Takeaway.com   \n",
       "1                                      Spotcap Global   \n",
       "2                                              komoot   \n",
       "3                                                 N26   \n",
       "4                                              Amazon   \n",
       "5                                               Jimdo   \n",
       "6                                             AiSight   \n",
       "7                            Euro London Appointments   \n",
       "8                                               Adobe   \n",
       "9                                            Glocomms   \n",
       "10                                              Bayer   \n",
       "11                                        The REC HUB   \n",
       "12                                               ICIS   \n",
       "13                                       GetYourGuide   \n",
       "14                                            trivago   \n",
       "15                                              Alcon   \n",
       "16                                             Mintel   \n",
       "17                                           CID GmbH   \n",
       "18                                            Lab1886   \n",
       "19                                          Movement8   \n",
       "20                                             Nubank   \n",
       "21                                  HERE Technologies   \n",
       "22                                             eClerx   \n",
       "23                                      Delivery Hero   \n",
       "24                                             Airbus   \n",
       "25                     CAMELOT Management Consultants   \n",
       "26                                            Avanade   \n",
       "27                                       Takeaway.com   \n",
       "28                                 McKinsey & Company   \n",
       "29                                      Camelot ITLab   \n",
       "30                                             Amazon   \n",
       "31                                              Apple   \n",
       "32                                              Bayer   \n",
       "33                                              Jimdo   \n",
       "34                                                N26   \n",
       "35                           Euro London Appointments   \n",
       "36                                            Joonko    \n",
       "37                                  HERE Technologies   \n",
       "38                                       EPAM Systems   \n",
       "39                                           Jam City   \n",
       "40                                     Cape Analytics   \n",
       "41  Global Legal Entity Identifier Foundation (GLEIF)   \n",
       "42                                     The Tron Group   \n",
       "43                           Lendico Deutschland GmbH   \n",
       "44              NeuroNation - Brain Health Technology   \n",
       "45                                              Agoda   \n",
       "46                                 Frontier Car Group   \n",
       "47                                 human council gmbh   \n",
       "48                                   Schwäbisch Media   \n",
       "49                                         Beiersdorf   \n",
       "\n",
       "                                 Location  \n",
       "0                              Berlin, DE  \n",
       "1        Berlin und Umgebung, Deutschland  \n",
       "2                 Berlin, Berlin, Germany  \n",
       "3                              Berlin, DE  \n",
       "4                              Munich, DE  \n",
       "5               Hamburg, Hamburg, Germany  \n",
       "6                                  Berlin  \n",
       "7                 Berlin, Berlin, Germany  \n",
       "8                             Hamburg, DE  \n",
       "9                 Stuttgart Area, Germany  \n",
       "10                   Monheim am Rhein, DE  \n",
       "11                   Berlin Area, Germany  \n",
       "12  Karlsruhe, Baden-Württemberg, Germany  \n",
       "13                             Berlin, DE  \n",
       "14                         Düsseldorf, DE  \n",
       "15                             Berlin, DE  \n",
       "16                         Düsseldorf, DE  \n",
       "17       Freigericht, Hessen, Deutschland  \n",
       "18                   Berlin Area, Germany  \n",
       "19                   Berlin Area, Germany  \n",
       "20                             Berlin, DE  \n",
       "21                             Berlin, DE  \n",
       "22                     Herzogenaurach, DE  \n",
       "23                             Berlin, DE  \n",
       "24        Donauwörth, Bayern, Deutschland  \n",
       "25   Mannheim, Baden-Württemberg, Germany  \n",
       "26                         Düsseldorf, DE  \n",
       "27                             Berlin, DE  \n",
       "28                             Berlin, DE  \n",
       "29   Mannheim, Baden-Württemberg, Germany  \n",
       "30                             Munich, DE  \n",
       "31                             Munich, DE  \n",
       "32                   Monheim am Rhein, DE  \n",
       "33              Hamburg, Hamburg, Germany  \n",
       "34                             Berlin, DE  \n",
       "35                Berlin, Berlin, Germany  \n",
       "36                Berlin, Berlin, Germany  \n",
       "37                             Berlin, DE  \n",
       "38                  Frankfurt am Main, DE  \n",
       "39                             Berlin, DE  \n",
       "40                             Munich, DE  \n",
       "41                  Frankfurt am Main, DE  \n",
       "42                              Mainz, DE  \n",
       "43                             Berlin, DE  \n",
       "44                   Berlin Area, Germany  \n",
       "45                             Munich, DE  \n",
       "46                             Berlin, DE  \n",
       "47                             Munich, DE  \n",
       "48                          Stuttgart, DE  \n",
       "49                            Hamburg, DE  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scrape_linkedin_job_search3('data%20analysis',2,'germany')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge 3\n",
    "\n",
    "Add the 4th param called `num_days` to your function to allow it to search jobs posted in the past X days. Note that in the LinkedIn job search the searched timespan is specified with the following param:\n",
    "\n",
    "```\n",
    "f_TPR=r259200\n",
    "```\n",
    "\n",
    "The number part in the param value is the number of seconds. 259,200 seconds equal to 3 days. You need to convert `num_days` to number of seconds and supply that info to LinkedIn job search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "# your code here\n",
    "# Import the required libraries\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "\"\"\"\n",
    "This function searches job posts from LinkedIn and converts the results into a dataframe.\n",
    "\"\"\"\n",
    "def scrape_linkedin_job_search4(keywords,numpages,country,num_days):\n",
    "    \n",
    "    # Define the base url to be scraped.\n",
    "    # All uppercase variable name signifies this is a constant and its value should never unchange\n",
    "    BASE_URL = 'https://www.linkedin.com/jobs/search/?'\n",
    "    \n",
    "    # Create an empty dataframe with the columns consisting of the information you want to capture\n",
    "    columns = ['Title', 'Company', 'Location']\n",
    "    data = pd.DataFrame(columns=columns)\n",
    "    num_secs=num_days*3600\n",
    "    for x in range(numpages):\n",
    "        numpages_num=x*25\n",
    "        # Assemble the full url with parameters\n",
    "        scrape_url = ''.join([BASE_URL,'f_TPR=r',str(num_secs) ,'keywords=', keywords,'&start=',str(numpages_num),'&location=',country])\n",
    "\n",
    "        # Create a request to get the data from the server \n",
    "        page = requests.get(scrape_url)\n",
    "        soup = BeautifulSoup(page.text, 'html.parser')\n",
    "\n",
    "        # Retrieve HTML code from the webpage. Parse the HTML into a list of \"cards\".\n",
    "        # Then in each job card, extract the job title, company, and location data.\n",
    "        titles = []\n",
    "        companies = []\n",
    "        locations = []\n",
    "        for card in soup.select(\"div.result-card__contents\"):\n",
    "            title = card.findChild(\"h3\", recursive=False)\n",
    "            company = card.findChild(\"h4\", recursive=False)\n",
    "            location = card.findChild(\"span\", attrs={\"class\": \"job-result-card__location\"}, recursive=True)\n",
    "            titles.append(title.string)\n",
    "            companies.append(company.string)\n",
    "            locations.append(location.string)\n",
    "\n",
    "        # Inject job titles, companies, and locations into the empty dataframe\n",
    "        zipped = zip(titles, companies, locations)\n",
    "        for z in list(zipped):\n",
    "            data=data.append({'Title' : z[0] , 'Company' : z[1], 'Location': z[2]} , ignore_index=True)\n",
    "\n",
    "    # Return dataframe\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Company</th>\n",
       "      <th>Location</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>Maschineneinrichter (m/w/d) in der pharmazeuti...</td>\n",
       "      <td>Pfizer</td>\n",
       "      <td>Freiburg, DE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>Technischer Einkäufer Laborautomation (m/w/d)</td>\n",
       "      <td>Abbott</td>\n",
       "      <td>Hamburg, DE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>Praktikant Human Resources Development - Autom...</td>\n",
       "      <td>BorgWarner Inc.</td>\n",
       "      <td>Kirchheimbolanden, DE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>Data Manager (m/w/d)</td>\n",
       "      <td>Bisnode</td>\n",
       "      <td>Darmstadt, DE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>Duales Studium Wirtschaftsingenieurwesen (m/w/d)</td>\n",
       "      <td>ANDRITZ</td>\n",
       "      <td>Dachau, DE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>Wissenschaftliche*r Mitarbeiter*in für ein int...</td>\n",
       "      <td>Hochschule Ruhr West University of Applied Sci...</td>\n",
       "      <td>Mülheim an der Ruhr, 45479, DE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>Business Intelligence Consultant</td>\n",
       "      <td>Klara Consulting Ltd</td>\n",
       "      <td>Munich, Bavaria, Germany</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>Manager IT Infrastruktur (m/w/d)</td>\n",
       "      <td>HAUTAU GmbH</td>\n",
       "      <td>Helpsen, Niedersachsen, Deutschland</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>HR Working Student (m/f)</td>\n",
       "      <td>relayr</td>\n",
       "      <td>Berlin Area, Germany</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>General Manager Ultrasound IT (Viewpoint) (m/w...</td>\n",
       "      <td>GE Healthcare</td>\n",
       "      <td>Munich Area, Germany</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>Junior Client Service Executive Outwards (m/w/d)</td>\n",
       "      <td>Chubb</td>\n",
       "      <td>Frankfurt am Main, DE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>Head of Precision Medicine Markers (m/f/d)</td>\n",
       "      <td>Bayer</td>\n",
       "      <td>Wuppertal, DE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>Project Purchaser</td>\n",
       "      <td>Britax Römer Child Safety EMEA</td>\n",
       "      <td>Leipheim, Bavaria, Germany</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>Sales Administrator (m/f/d)</td>\n",
       "      <td>Bulgari</td>\n",
       "      <td>München, Bayern, Deutschland</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>Copywriter</td>\n",
       "      <td>AKQA</td>\n",
       "      <td>Berlin und Umgebung, Deutschland</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>Oracle Developer</td>\n",
       "      <td>HCL Technologies</td>\n",
       "      <td>Frankfurt Am Main Area, Germany</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>(Senior) Director (m/w/d) - Transformation</td>\n",
       "      <td>Alvarez &amp; Marsal</td>\n",
       "      <td>Frankfurt am Main und Umgebung, Deutschland</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>Mechanical Engineer (w/m/d) Process Systems fü...</td>\n",
       "      <td>Beiersdorf</td>\n",
       "      <td>Hamburg, DE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>Shopmitarbeiter (m/w/d) auf Minijob-Basis</td>\n",
       "      <td>Tipico</td>\n",
       "      <td>Chemnitz, DE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>Sales Advisor (m/w/d) auf flexibler Stundenbas...</td>\n",
       "      <td>H&amp;M</td>\n",
       "      <td>Bonn, DE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>Test &amp; Verification Architect Embedded Softwar...</td>\n",
       "      <td>Punch Powertrain</td>\n",
       "      <td>Munich, DE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>Studioleiter (m/w/d) für McFIT Großraum Erlang...</td>\n",
       "      <td>McFIT Global Group GmbH</td>\n",
       "      <td>Erlangen, DE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>Manager in Training</td>\n",
       "      <td>Hollister Co.</td>\n",
       "      <td>Koblenz, DE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>Recruiter Active Sourcing</td>\n",
       "      <td>Testbirds</td>\n",
       "      <td>Munich, DE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>Facharzt Notfallmedizin Management Adac Luftre...</td>\n",
       "      <td>ADAC</td>\n",
       "      <td>Munich, DE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>Technischer Einkäufer Laborautomation (m/w/d)</td>\n",
       "      <td>Abbott</td>\n",
       "      <td>Hamburg, DE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>Data Manager (m/w/d)</td>\n",
       "      <td>Bisnode</td>\n",
       "      <td>Darmstadt, DE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>Praktikant Human Resources Development - Autom...</td>\n",
       "      <td>BorgWarner Inc.</td>\n",
       "      <td>Kirchheimbolanden, DE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>Maschineneinrichter (m/w/d) in der pharmazeuti...</td>\n",
       "      <td>Pfizer</td>\n",
       "      <td>Freiburg, DE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>Wissenschaftliche*r Mitarbeiter*in für ein int...</td>\n",
       "      <td>Hochschule Ruhr West University of Applied Sci...</td>\n",
       "      <td>Mülheim an der Ruhr, 45479, DE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>Manager IT Infrastruktur (m/w/d)</td>\n",
       "      <td>HAUTAU GmbH</td>\n",
       "      <td>Helpsen, Niedersachsen, Deutschland</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>Business Intelligence Consultant</td>\n",
       "      <td>Klara Consulting Ltd</td>\n",
       "      <td>Munich, Bavaria, Germany</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>(Senior) Director (m/w/d) - Transformation</td>\n",
       "      <td>Alvarez &amp; Marsal</td>\n",
       "      <td>Frankfurt am Main und Umgebung, Deutschland</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>PLM Solution Architekt (f/m/d)</td>\n",
       "      <td>PTC</td>\n",
       "      <td>Hamburg Area, Germany</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>Junior-Marketingmanager (m/w/d)</td>\n",
       "      <td>iMedia888 GmbH</td>\n",
       "      <td>München, Bayern, Deutschland</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>(Senior) Clinical Research Associate (m/w)</td>\n",
       "      <td>DOCS</td>\n",
       "      <td>Berlin und Umgebung, Deutschland</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>Field Service Coordinator</td>\n",
       "      <td>Lectra</td>\n",
       "      <td>Munich, Bavaria, Germany</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>Project Purchaser</td>\n",
       "      <td>Britax Römer Child Safety EMEA</td>\n",
       "      <td>Leipheim, Bavaria, Germany</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>Product Segment Manager - Building and Industry</td>\n",
       "      <td>Glatfelter</td>\n",
       "      <td>Germany</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>Site Manager-Europe</td>\n",
       "      <td>Hayes Performance Systems</td>\n",
       "      <td>Munich, Bavaria, Germany</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>General Manager Ultrasound IT (Viewpoint) (m/w...</td>\n",
       "      <td>GE Healthcare</td>\n",
       "      <td>Munich Area, Germany</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41</td>\n",
       "      <td>Sales Administrator (m/f/d)</td>\n",
       "      <td>Bulgari</td>\n",
       "      <td>München, Bayern, Deutschland</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42</td>\n",
       "      <td>Oracle Developer</td>\n",
       "      <td>HCL Technologies</td>\n",
       "      <td>Frankfurt Am Main Area, Germany</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43</td>\n",
       "      <td>Sales Representative (equipment and services)</td>\n",
       "      <td>STERIS</td>\n",
       "      <td>Hamburg Area, Germany</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44</td>\n",
       "      <td>Systemspezialist (w/m/d) - Erfurt</td>\n",
       "      <td>Bechtle</td>\n",
       "      <td>Erfurt, DE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>Ingenieur (m/w/d) Elektrotechnik, Kommunikatio...</td>\n",
       "      <td>Bundesamt für Seeschifffahrt und Hydrographie</td>\n",
       "      <td>Hamburg, DE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46</td>\n",
       "      <td>Systementwickler (m/w/d) - Sinzheim</td>\n",
       "      <td>RAUCH Landmaschinenfabrik GmbH</td>\n",
       "      <td>Sinzheim, DE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47</td>\n",
       "      <td>angehende Führungskraft gesucht! - Dortmund</td>\n",
       "      <td>NRW</td>\n",
       "      <td>Dortmund, DE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>Tagetik Experte Planning &amp; Controlling (m/w/d)</td>\n",
       "      <td>1st solution consulting gmbh</td>\n",
       "      <td>Düsseldorf, DE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49</td>\n",
       "      <td>Finanzbuchhalter / in in Göttingen, Niedersach...</td>\n",
       "      <td>Prowork</td>\n",
       "      <td>Göttingen, DE</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Title  \\\n",
       "0   Maschineneinrichter (m/w/d) in der pharmazeuti...   \n",
       "1       Technischer Einkäufer Laborautomation (m/w/d)   \n",
       "2   Praktikant Human Resources Development - Autom...   \n",
       "3                                Data Manager (m/w/d)   \n",
       "4    Duales Studium Wirtschaftsingenieurwesen (m/w/d)   \n",
       "5   Wissenschaftliche*r Mitarbeiter*in für ein int...   \n",
       "6                    Business Intelligence Consultant   \n",
       "7                    Manager IT Infrastruktur (m/w/d)   \n",
       "8                            HR Working Student (m/f)   \n",
       "9   General Manager Ultrasound IT (Viewpoint) (m/w...   \n",
       "10   Junior Client Service Executive Outwards (m/w/d)   \n",
       "11         Head of Precision Medicine Markers (m/f/d)   \n",
       "12                                  Project Purchaser   \n",
       "13                        Sales Administrator (m/f/d)   \n",
       "14                                         Copywriter   \n",
       "15                                   Oracle Developer   \n",
       "16         (Senior) Director (m/w/d) - Transformation   \n",
       "17  Mechanical Engineer (w/m/d) Process Systems fü...   \n",
       "18          Shopmitarbeiter (m/w/d) auf Minijob-Basis   \n",
       "19  Sales Advisor (m/w/d) auf flexibler Stundenbas...   \n",
       "20  Test & Verification Architect Embedded Softwar...   \n",
       "21  Studioleiter (m/w/d) für McFIT Großraum Erlang...   \n",
       "22                                Manager in Training   \n",
       "23                          Recruiter Active Sourcing   \n",
       "24  Facharzt Notfallmedizin Management Adac Luftre...   \n",
       "25      Technischer Einkäufer Laborautomation (m/w/d)   \n",
       "26                               Data Manager (m/w/d)   \n",
       "27  Praktikant Human Resources Development - Autom...   \n",
       "28  Maschineneinrichter (m/w/d) in der pharmazeuti...   \n",
       "29  Wissenschaftliche*r Mitarbeiter*in für ein int...   \n",
       "30                   Manager IT Infrastruktur (m/w/d)   \n",
       "31                   Business Intelligence Consultant   \n",
       "32         (Senior) Director (m/w/d) - Transformation   \n",
       "33                     PLM Solution Architekt (f/m/d)   \n",
       "34                    Junior-Marketingmanager (m/w/d)   \n",
       "35         (Senior) Clinical Research Associate (m/w)   \n",
       "36                          Field Service Coordinator   \n",
       "37                                  Project Purchaser   \n",
       "38    Product Segment Manager - Building and Industry   \n",
       "39                                Site Manager-Europe   \n",
       "40  General Manager Ultrasound IT (Viewpoint) (m/w...   \n",
       "41                        Sales Administrator (m/f/d)   \n",
       "42                                   Oracle Developer   \n",
       "43      Sales Representative (equipment and services)   \n",
       "44                  Systemspezialist (w/m/d) - Erfurt   \n",
       "45  Ingenieur (m/w/d) Elektrotechnik, Kommunikatio...   \n",
       "46                Systementwickler (m/w/d) - Sinzheim   \n",
       "47        angehende Führungskraft gesucht! - Dortmund   \n",
       "48     Tagetik Experte Planning & Controlling (m/w/d)   \n",
       "49  Finanzbuchhalter / in in Göttingen, Niedersach...   \n",
       "\n",
       "                                              Company  \\\n",
       "0                                              Pfizer   \n",
       "1                                              Abbott   \n",
       "2                                     BorgWarner Inc.   \n",
       "3                                             Bisnode   \n",
       "4                                             ANDRITZ   \n",
       "5   Hochschule Ruhr West University of Applied Sci...   \n",
       "6                                Klara Consulting Ltd   \n",
       "7                                         HAUTAU GmbH   \n",
       "8                                              relayr   \n",
       "9                                       GE Healthcare   \n",
       "10                                              Chubb   \n",
       "11                                              Bayer   \n",
       "12                     Britax Römer Child Safety EMEA   \n",
       "13                                            Bulgari   \n",
       "14                                               AKQA   \n",
       "15                                   HCL Technologies   \n",
       "16                                   Alvarez & Marsal   \n",
       "17                                         Beiersdorf   \n",
       "18                                             Tipico   \n",
       "19                                                H&M   \n",
       "20                                   Punch Powertrain   \n",
       "21                            McFIT Global Group GmbH   \n",
       "22                                      Hollister Co.   \n",
       "23                                          Testbirds   \n",
       "24                                               ADAC   \n",
       "25                                             Abbott   \n",
       "26                                            Bisnode   \n",
       "27                                    BorgWarner Inc.   \n",
       "28                                             Pfizer   \n",
       "29  Hochschule Ruhr West University of Applied Sci...   \n",
       "30                                        HAUTAU GmbH   \n",
       "31                               Klara Consulting Ltd   \n",
       "32                                   Alvarez & Marsal   \n",
       "33                                                PTC   \n",
       "34                                     iMedia888 GmbH   \n",
       "35                                               DOCS   \n",
       "36                                             Lectra   \n",
       "37                     Britax Römer Child Safety EMEA   \n",
       "38                                         Glatfelter   \n",
       "39                          Hayes Performance Systems   \n",
       "40                                      GE Healthcare   \n",
       "41                                            Bulgari   \n",
       "42                                   HCL Technologies   \n",
       "43                                             STERIS   \n",
       "44                                            Bechtle   \n",
       "45      Bundesamt für Seeschifffahrt und Hydrographie   \n",
       "46                     RAUCH Landmaschinenfabrik GmbH   \n",
       "47                                                NRW   \n",
       "48                       1st solution consulting gmbh   \n",
       "49                                            Prowork   \n",
       "\n",
       "                                       Location  \n",
       "0                                  Freiburg, DE  \n",
       "1                                   Hamburg, DE  \n",
       "2                         Kirchheimbolanden, DE  \n",
       "3                                 Darmstadt, DE  \n",
       "4                                    Dachau, DE  \n",
       "5                Mülheim an der Ruhr, 45479, DE  \n",
       "6                      Munich, Bavaria, Germany  \n",
       "7           Helpsen, Niedersachsen, Deutschland  \n",
       "8                          Berlin Area, Germany  \n",
       "9                          Munich Area, Germany  \n",
       "10                        Frankfurt am Main, DE  \n",
       "11                                Wuppertal, DE  \n",
       "12                   Leipheim, Bavaria, Germany  \n",
       "13                 München, Bayern, Deutschland  \n",
       "14             Berlin und Umgebung, Deutschland  \n",
       "15              Frankfurt Am Main Area, Germany  \n",
       "16  Frankfurt am Main und Umgebung, Deutschland  \n",
       "17                                  Hamburg, DE  \n",
       "18                                 Chemnitz, DE  \n",
       "19                                     Bonn, DE  \n",
       "20                                   Munich, DE  \n",
       "21                                 Erlangen, DE  \n",
       "22                                  Koblenz, DE  \n",
       "23                                   Munich, DE  \n",
       "24                                   Munich, DE  \n",
       "25                                  Hamburg, DE  \n",
       "26                                Darmstadt, DE  \n",
       "27                        Kirchheimbolanden, DE  \n",
       "28                                 Freiburg, DE  \n",
       "29               Mülheim an der Ruhr, 45479, DE  \n",
       "30          Helpsen, Niedersachsen, Deutschland  \n",
       "31                     Munich, Bavaria, Germany  \n",
       "32  Frankfurt am Main und Umgebung, Deutschland  \n",
       "33                        Hamburg Area, Germany  \n",
       "34                 München, Bayern, Deutschland  \n",
       "35             Berlin und Umgebung, Deutschland  \n",
       "36                     Munich, Bavaria, Germany  \n",
       "37                   Leipheim, Bavaria, Germany  \n",
       "38                                      Germany  \n",
       "39                     Munich, Bavaria, Germany  \n",
       "40                         Munich Area, Germany  \n",
       "41                 München, Bayern, Deutschland  \n",
       "42              Frankfurt Am Main Area, Germany  \n",
       "43                        Hamburg Area, Germany  \n",
       "44                                   Erfurt, DE  \n",
       "45                                  Hamburg, DE  \n",
       "46                                 Sinzheim, DE  \n",
       "47                                 Dortmund, DE  \n",
       "48                               Düsseldorf, DE  \n",
       "49                                Göttingen, DE  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scrape_linkedin_job_search4('data%20analysis',2,'germany',7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus Challenge\n",
    "\n",
    "Allow your function to also retrieve the \"Seniority Level\" of each job searched. Note that the Seniority Level info is not in the initial search results. You need to make a separate search request for each job card based on the `currentJobId` value which you can extract from the job card HTML.\n",
    "\n",
    "After you obtain the Seniority Level info, update the function and add it to a new column of the returned dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "# your code here\n",
    "# Import the required libraries\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "\"\"\"\n",
    "This function searches job posts from LinkedIn and converts the results into a dataframe.\n",
    "\"\"\"\n",
    "def scrape_linkedin_job_search5(keywords,numpages,country,num_days):\n",
    "    \n",
    "    # Define the base url to be scraped.\n",
    "    # All uppercase variable name signifies this is a constant and its value should never unchange\n",
    "    BASE_URL = 'https://www.linkedin.com/jobs/search/?'\n",
    "    \n",
    "    # Create an empty dataframe with the columns consisting of the information you want to capture\n",
    "    columns = ['Title', 'Company', 'Location']\n",
    "    data = pd.DataFrame(columns=columns)\n",
    "    num_secs=num_days*3600\n",
    "    for x in range(numpages):\n",
    "        numpages_num=x*25\n",
    "        # Assemble the full url with parameters\n",
    "        scrape_url = ''.join([BASE_URL,'f_TPR=r',str(num_secs) ,'keywords=', keywords,'&start=',str(numpages_num),'&location=',country])\n",
    "\n",
    "        # Create a request to get the data from the server \n",
    "        page = requests.get(scrape_url)\n",
    "        soup = BeautifulSoup(page.text, 'html.parser')\n",
    "        \n",
    "        # Retrieve HTML code from the webpage. Parse the HTML into a list of \"cards\".\n",
    "        # Then in each job card, extract the job title, company, and location data.\n",
    "        titles = []\n",
    "        companies = []\n",
    "        locations = []\n",
    "        seniority_level=[]\n",
    "        for card in soup.select(\"div.result-card__contents\"):\n",
    "            title = card.findChild(\"h3\", recursive=False)\n",
    "            company = card.findChild(\"h4\", recursive=False)\n",
    "            location = card.findChild(\"span\", attrs={\"class\": \"job-result-card__location\"}, recursive=True)\n",
    "            titles.append(title.string)\n",
    "            companies.append(company.string)\n",
    "            locations.append(location.string)\n",
    "            #print(card.find(\"a\").attrs['href'])\n",
    "            #print(card.a)\n",
    "        \n",
    "        #print(soup.body.find(\"div\",class_=\"jobs-search-two-pane__results display-flex\"))\n",
    "        #for link in soup.find(\"div\",class_=\"job-card-search--two-pane  jobs-search-results__list--card--viewport-tracking-2 job-card-search job-card-search--column job-card-search ember-view\"):\n",
    "        #    print(link)\n",
    "        #    if 'href' in link.attrs:\n",
    "        #        print(link.attrs['href'])\n",
    "        #    \n",
    "        sen_level=\"none\"\n",
    "        #print(sen_level)\n",
    "            \n",
    "            \n",
    "            #currentJobId=1510822801         \n",
    "        seniority_level.append(sen_level)\n",
    "        \n",
    "        \n",
    "        # Inject job titles, companies, and locations into the empty dataframe\n",
    "        zipped = zip(titles, companies, locations,seniority_level)\n",
    "        for z in list(zipped):\n",
    "            data=data.append({'Title' : z[0] , 'Company' : z[1], 'Location': z[2], 'Seniority_Level': z[3]} , ignore_index=True)\n",
    "    \n",
    "    # Return dataframe\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "None\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Company</th>\n",
       "      <th>Location</th>\n",
       "      <th>Seniority_Level</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>Project Finance Account Manager</td>\n",
       "      <td>Medpace</td>\n",
       "      <td>Munich, DE</td>\n",
       "      <td>none</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>Project Finance Account Manager</td>\n",
       "      <td>Medpace</td>\n",
       "      <td>Munich, DE</td>\n",
       "      <td>none</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             Title  Company    Location Seniority_Level\n",
       "0  Project Finance Account Manager  Medpace  Munich, DE            none\n",
       "1  Project Finance Account Manager  Medpace  Munich, DE            none"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# your code here\n",
    "\n",
    "scrape_linkedin_job_search5('data%20analysis',2,'germany',1)\n",
    "\n",
    "#Mid-Senior level\n",
    "#class=\"jobs-box__body js-formatted-exp-body\"\n",
    "\n",
    "#class=\"jobs-box__body js-formatted-exp-body\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "range(0, 2)"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "range(2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
